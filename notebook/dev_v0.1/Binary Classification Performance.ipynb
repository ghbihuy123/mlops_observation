{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HuyLQ15_CTV\\Desktop\\model_monitoring\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HuyLQ15_CTV\\AppData\\Local\\miniconda3\\envs\\model-monitoring\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data = load_breast_cancer(as_frame=True)['data']\n",
    "data['target'] = load_breast_cancer(as_frame=True)['target']\n",
    "\n",
    "X = data.drop('target', axis=1).iloc[:125]\n",
    "y = data['target'].iloc[:125]\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "prediction1 = model.predict(data.drop('target', axis=1))\n",
    "prediction2 = model.predict_proba(data.drop('target', axis=1))\n",
    "data['prediction'] = prediction1\n",
    "for col, value in zip(model.classes_, prediction2.transpose()):\n",
    "    data[col] = value\n",
    "data.columns = data.columns.map(str)\n",
    "\n",
    "reference = data.iloc[:200]\n",
    "current = data.iloc[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.pipeline.column_mapping import ColumnMapping\n",
    "from evidently.utils.data_operations import process_columns\n",
    "from evidently.calculations.classification_performance import get_prediction_data\n",
    "from source.metric_results.classification.confusion_matrix import calculate_confusion_matrix_by_classes\n",
    "column_mapping = ColumnMapping()\n",
    "column_mapping.target = 'target'\n",
    "column_mapping.prediction = '1'\n",
    "column_mapping.pos_label = 1\n",
    "\n",
    "dataset_column = process_columns(data, column_mapping)\n",
    "prediction_data = get_prediction_data(data, data_columns=dataset_column, pos_label=1, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from source.metric_results.classification.confusion_matrix import calculate_confusion_by_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data[column_mapping.target]\n",
    "prediction = prediction_data.predictions\n",
    "prediction_probas = prediction_data.prediction_probas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(target, prediction)\n",
    "roc_auc = roc_auc_score(target, prediction_probas[column_mapping.pos_label], multi_class='ovr')\n",
    "# accuracy\n",
    "# log_loss(target, prediction_probas[column_mapping.pos_label])\n",
    "classification_rp = classification_report(target, prediction, output_dict=True)\n",
    "# classification_rp.pop('accuracy')\n",
    "# calculate_confusion_matrix_by_classes(target, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Binary Classification Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationResult(type='evidently:metric_result:MultiClassificationResult', accuracy=0.9209138840070299, confusion_matrix={'0': {'tp': 207.0, 'tn': 317.0, 'fp': 40.0, 'fn': 5.0}, '1': {'tp': 317.0, 'tn': 207.0, 'fp': 5.0, 'fn': 40.0}}, classification_info={'0': {'precision': 0.8380566801619433, 'recall': 0.9764150943396226, 'f1-score': 0.9019607843137255, 'support': 212.0}, '1': {'precision': 0.984472049689441, 'recall': 0.8879551820728291, 'f1-score': 0.9337260677466863, 'support': 357.0}, 'macro avg': {'precision': 0.9112643649256922, 'recall': 0.9321851382062258, 'f1-score': 0.917843426030206, 'support': 569.0}, 'weighted avg': {'precision': 0.9299201018162784, 'recall': 0.9209138840070299, 'f1-score': 0.9218908479087466, 'support': 569.0}}, logloss=0.20927239133138018, roc_auc={'ovr': 0.9877517044553671, 'ovo': 0.9877517044553671}, mattheus_corr_coefficient=0.8431900056208114)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from source.metric_results.classification.classification_calculator import ClassificationResult\n",
    "from source.metric_results.classification.classification_calculator import calculate_classification_results\n",
    "\n",
    "calculate_classification_results(prediction, target, prediction_probas[column_mapping.pos_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationResult(type='evidently:metric_result:MultiClassificationResult', accuracy=0.8970189701897019, confusion_matrix={'0': {'tp': 106.0, 'tn': 225.0, 'fp': 36.0, 'fn': 2.0}, '1': {'tp': 225.0, 'tn': 106.0, 'fp': 2.0, 'fn': 36.0}}, classification_info={'0': {'precision': 0.7464788732394366, 'recall': 0.9814814814814815, 'f1-score': 0.848, 'support': 108.0}, '1': {'precision': 0.9911894273127754, 'recall': 0.8620689655172413, 'f1-score': 0.9221311475409836, 'support': 261.0}, 'macro avg': {'precision': 0.868834150276106, 'recall': 0.9217752234993615, 'f1-score': 0.8850655737704918, 'support': 369.0}, 'weighted avg': {'precision': 0.9195668261205786, 'recall': 0.8970189701897019, 'f1-score': 0.9004342263094762, 'support': 369.0}}, logloss=0.2682398653708357, roc_auc={'ovr': 0.987264084007379, 'ovo': 0.987264084007379}, mattheus_corr_coefficient=0.7888348525943859)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_classification_result(data: pd.DataFrame, column_mapping: ColumnMapping) -> ClassificationResult:\n",
    "        \n",
    "        dataset_column = process_columns(data, column_mapping)\n",
    "        prediction_data = get_prediction_data(data, data_columns=dataset_column, pos_label=None, threshold=0.5)\n",
    "        results = calculate_classification_results(\n",
    "            prediction=prediction_data.predictions,\n",
    "            target=data[dataset_column.utility_columns.target],\n",
    "            prediction_proba=prediction_data.prediction_probas[column_mapping.pos_label]\n",
    "        )\n",
    "        return results\n",
    "get_classification_result(current, column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Optional, Dict\n",
    "from evidently.calculations.classification_performance import get_prediction_data\n",
    "from evidently.renderers.base_renderer import MetricRenderer, default_renderer\n",
    "from evidently.base_metric import InputData, Metric, MetricResult\n",
    "from evidently.utils.data_operations import process_columns\n",
    "from evidently import ColumnMapping\n",
    "from evidently.base_metric import Metric\n",
    "from source.metric_results.classification.classification_calculator import calculate_classification_results\n",
    "\n",
    "class BinaryClassificationPerformanceResult(MetricResult):\n",
    "    class Config:\n",
    "        type_alias = \"evidently:metric_result:BinaryClassificationPerformanceResult\"\n",
    "    reference: Optional[ClassificationResult]\n",
    "    current: ClassificationResult\n",
    "\n",
    "class BinaryClassificationPerformanceMetric(Metric[ClassificationResult]):\n",
    "    class Config:\n",
    "        type_alias = \"evidently:metric:BinaryClassificationPerformanceMetric\"\n",
    "    _threshold: float\n",
    "    def __init__(self, threshold: float = 0.5):\n",
    "        self._threshold = threshold\n",
    "        super().__init__()\n",
    "\n",
    "    def calculate(self, data: InputData) -> ClassificationResult:\n",
    "        results = {}\n",
    "        results['reference'] = None\n",
    "        if data.current_data is None:\n",
    "            raise ValueError(\"The value cannot be None\")\n",
    "\n",
    "        if data.reference_data is not None:\n",
    "            results['reference'] = self.get_classification_result(data.reference_data, data.column_mapping)\n",
    "\n",
    "        results['current'] = self.get_classification_result(data.current_data, data.column_mapping)\n",
    "        return BinaryClassificationPerformanceResult(\n",
    "            reference=results['reference'],\n",
    "            current=results['current']\n",
    "        )\n",
    "        \n",
    "    def get_classification_result(self, data: pd.DataFrame, column_mapping: ColumnMapping) -> ClassificationResult:\n",
    "        \n",
    "        dataset_column = process_columns(data, column_mapping)\n",
    "        prediction_data = get_prediction_data(data, data_columns=dataset_column, pos_label=None, threshold=self._threshold)\n",
    "        results = calculate_classification_results(\n",
    "            prediction=prediction_data.predictions,\n",
    "            target=data[dataset_column.utility_columns.target],\n",
    "            prediction_proba=prediction_data.prediction_probas[column_mapping.pos_label]\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'default_renderer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrenderers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtml_widgets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseWidgetInfo\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;129m@default_renderer\u001b[39m(wrap_type\u001b[38;5;241m=\u001b[39mBinaryClassificationPerformanceMetric)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBinaryClassificationPerformanceRender\u001b[39;00m(MetricRenderer):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: BinaryClassificationPerformanceMetric, include_render: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m         include: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncludeOptions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, exclude: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncludeOptions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m      7\u001b[0m         result \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mget_result()\u001b[38;5;241m.\u001b[39mget_dict(include_render, include, exclude)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'default_renderer' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from evidently.renderers.html_widgets import BaseWidgetInfo\n",
    "from evidently.renderers.base_renderer import MetricRenderer, default_renderer\n",
    "\n",
    "\n",
    "@default_renderer(wrap_type=BinaryClassificationPerformanceMetric)\n",
    "class BinaryClassificationPerformanceRender(MetricRenderer):\n",
    "    def render_json(self, obj: BinaryClassificationPerformanceMetric, include_render: bool = False,\n",
    "        include: \"IncludeOptions\" = None, exclude: \"IncludeOptions\" = None,) -> dict:\n",
    "        result = obj.get_result().get_dict(include_render, include, exclude)\n",
    "        return result\n",
    "    def render_html(self, obj: BinaryClassificationPerformanceMetric) -> List[BaseWidgetInfo]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"version\": \"0.4.39\", \"metrics\": [{\"metric\": \"BinaryClassificationPerformanceMetric\", \"result\": {\"reference\": null, \"current\": {\"accuracy\": 0.9209138840070299, \"confusion_matrix\": {\"0\": {\"tp\": 207.0, \"tn\": 317.0, \"fp\": 40.0, \"fn\": 5.0}, \"1\": {\"tp\": 317.0, \"tn\": 207.0, \"fp\": 5.0, \"fn\": 40.0}}, \"classification_info\": {\"0\": {\"precision\": 0.8380566801619433, \"recall\": 0.9764150943396226, \"f1-score\": 0.9019607843137255, \"support\": 212.0}, \"1\": {\"precision\": 0.984472049689441, \"recall\": 0.8879551820728291, \"f1-score\": 0.9337260677466863, \"support\": 357.0}, \"macro avg\": {\"precision\": 0.9112643649256922, \"recall\": 0.9321851382062258, \"f1-score\": 0.917843426030206, \"support\": 569.0}, \"weighted avg\": {\"precision\": 0.9299201018162784, \"recall\": 0.9209138840070299, \"f1-score\": 0.9218908479087466, \"support\": 569.0}}, \"logloss\": 0.20927239133138018, \"roc_auc\": {\"ovr\": 0.9877517044553671, \"ovo\": 0.9877517044553671}, \"mattheus_corr_coefficient\": 0.8431900056208114}}}], \"timestamp\": \"2024-11-20 11:48:35.796984\"}'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evidently.report import Report\n",
    "\n",
    "report = Report(metrics=[\n",
    "    BinaryClassificationPerformanceMetric()\n",
    "])\n",
    "\n",
    "report.run(current_data=data, reference_data=None, column_mapping=column_mapping)\n",
    "report.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HuyLQ15_CTV\\AppData\\Local\\miniconda3\\envs\\model-monitoring\\lib\\site-packages\\evidently\\pydantic_utils.py:161: UserWarning: Duplicate key (<class 'evidently.base_metric.Metric'>, 'evidently:metric:RegressionPerformanceMetrics') in alias map\n",
      "  warnings.warn(f\"Duplicate key {key} in alias map\")\n",
      "c:\\Users\\HuyLQ15_CTV\\AppData\\Local\\miniconda3\\envs\\model-monitoring\\lib\\site-packages\\evidently\\pydantic_utils.py:161: UserWarning: Duplicate key (<class 'evidently.base_metric.MetricResult'>, 'evidently:metric_result:BinaryClassificationPerformanceResult') in alias map\n",
      "  warnings.warn(f\"Duplicate key {key} in alias map\")\n",
      "c:\\Users\\HuyLQ15_CTV\\AppData\\Local\\miniconda3\\envs\\model-monitoring\\lib\\site-packages\\evidently\\pydantic_utils.py:161: UserWarning: Duplicate key (<class 'evidently.base_metric.Metric'>, 'evidently:metric:BinaryClassificationPerformanceMetric') in alias map\n",
      "  warnings.warn(f\"Duplicate key {key} in alias map\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"version\": \"0.4.39\", \"metrics\": [{\"metric\": \"BinaryClassificationPerformanceMetric\", \"result\": {\"reference\": null, \"current\": {\"accuracy\": 0.9209138840070299, \"confusion_matrix\": {\"0\": {\"tp\": 207.0, \"tn\": 317.0, \"fp\": 40.0, \"fn\": 5.0}, \"1\": {\"tp\": 317.0, \"tn\": 207.0, \"fp\": 5.0, \"fn\": 40.0}}, \"classification_info\": {\"0\": {\"precision\": 0.8380566801619433, \"recall\": 0.9764150943396226, \"f1-score\": 0.9019607843137255, \"support\": 212.0}, \"1\": {\"precision\": 0.984472049689441, \"recall\": 0.8879551820728291, \"f1-score\": 0.9337260677466863, \"support\": 357.0}, \"macro avg\": {\"precision\": 0.9112643649256922, \"recall\": 0.9321851382062258, \"f1-score\": 0.917843426030206, \"support\": 569.0}, \"weighted avg\": {\"precision\": 0.9299201018162784, \"recall\": 0.9209138840070299, \"f1-score\": 0.9218908479087466, \"support\": 569.0}}, \"logloss\": 0.20927239133138018, \"roc_auc\": {\"ovr\": 0.9877517044553671, \"ovo\": 0.9877517044553671}, \"mattheus_corr_coefficient\": 0.8431900056208114}}}], \"timestamp\": \"2024-11-20 11:48:36.063603\"}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evidently.report import Report\n",
    "from source.metrics.classification_performance.binary_classification_performance import BinaryClassificationPerformanceMetric\n",
    "\n",
    "report = Report(metrics=[\n",
    "    BinaryClassificationPerformanceMetric()\n",
    "])\n",
    "\n",
    "report.run(current_data=data, reference_data=None, column_mapping=column_mapping)\n",
    "report.json()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "207e60eac2ff56558166198e2c0941ea4216ea025c42ddd95d3a76ed9cb3f5a9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.15 ('model-monitoring')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
